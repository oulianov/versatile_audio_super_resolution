diff --git a/app.py b/app.py
index a1d27bf..4a70e48 100644
--- a/app.py
+++ b/app.py
@@ -7,19 +7,22 @@ import numpy as np
 import soundfile as sf
 
 from audiosr import build_model, super_resolution
+from loguru import logger
 
 
 def detect_audio_end(audio, sr, window_size=2048, hop_length=512, threshold_db=-50):
     """Detect the end of actual audio content using RMS energy"""
     # Calculate RMS energy
-    rms = librosa.feature.rms(y=audio, frame_length=window_size, hop_length=hop_length)[0]
-    
+    rms = librosa.feature.rms(y=audio, frame_length=window_size, hop_length=hop_length)[
+        0
+    ]
+
     # Convert to dB
     db = librosa.amplitude_to_db(rms, ref=np.max)
-    
+
     # Find the last frame above threshold
     valid_frames = np.where(db > threshold_db)[0]
-    
+
     if len(valid_frames) > 0:
         last_frame = valid_frames[-1]
         # Convert frame index to sample index
@@ -27,90 +30,110 @@ def detect_audio_end(audio, sr, window_size=2048, hop_length=512, threshold_db=-
         return last_sample
     return len(audio)
 
+
 def calculate_amplitude_stats(audio):
     """Calculate amplitude statistics for audio normalization"""
     rms = np.sqrt(np.mean(np.square(audio)))
     peak = np.max(np.abs(audio))
     return rms, peak
 
+
 def normalize_chunk_amplitude(processed_chunk, original_chunk):
     """Normalize processed chunk to match original chunk's amplitude characteristics"""
     orig_rms, orig_peak = calculate_amplitude_stats(original_chunk)
     proc_rms, proc_peak = calculate_amplitude_stats(processed_chunk)
-    
+
     # Avoid division by zero
     if proc_rms < 1e-8:
         return processed_chunk
-    
+
     # Calculate scaling factor based on RMS ratio
     scale_factor = orig_rms / proc_rms
-    
+
     # Apply scaling while ensuring we don't exceed the original peak ratio
     peak_ratio = orig_peak / proc_peak if proc_peak > 0 else 1
     scale_factor = min(scale_factor, peak_ratio)
-    
+
     return processed_chunk * scale_factor
 
-def process_chunk(audiosr, chunk, sr, guidance_scale, ddim_steps, is_last_chunk=False, target_length=None):
+
+def process_chunk(
+    audiosr,
+    chunk,
+    sr,
+    guidance_scale,
+    ddim_steps,
+    is_last_chunk=False,
+    target_length=None,
+):
     # Create a temporary directory in the current working directory
     temp_dir = os.path.join(os.getcwd(), "temp_audio")
     os.makedirs(temp_dir, exist_ok=True)
-    
+
     # Create a unique temporary file path
     temp_path = os.path.join(temp_dir, f"chunk_{np.random.randint(0, 1000000)}.wav")
-    
+
     try:
         # Save chunk to temporary file
         sf.write(temp_path, chunk, sr)
-        
+
         # For the last chunk, adjust ddim_steps based on length
         if is_last_chunk:
             chunk_duration = len(chunk) / sr
             # Scale ddim_steps proportionally for shorter chunks, ensuring it stays within valid bounds
             # Subtract 2 to ensure we're well within the valid range (0 to ddim_steps-1)
-            max_steps = min(ddim_steps - 2, 998)  # Ensure we never exceed the valid range
-            adjusted_ddim_steps = max(10, min(max_steps, int(ddim_steps * (chunk_duration / 5.1))))
-            print(f"Adjusted ddim_steps for last chunk: {adjusted_ddim_steps}")
+            max_steps = min(
+                ddim_steps - 2, 998
+            )  # Ensure we never exceed the valid range
+            adjusted_ddim_steps = max(
+                10, min(max_steps, int(ddim_steps * (chunk_duration / 5.1)))
+            )
+            logger.info(f"Adjusted ddim_steps for last chunk: {adjusted_ddim_steps}")
         else:
-            adjusted_ddim_steps = min(ddim_steps - 2, 998)  # Also bound regular chunks for safety
-        
+            adjusted_ddim_steps = min(
+                ddim_steps - 2, 998
+            )  # Also bound regular chunks for safety
+
         # Process the chunk
         processed_chunk = super_resolution(
             audiosr,
             temp_path,
             guidance_scale=guidance_scale,
-            ddim_steps=adjusted_ddim_steps
+            ddim_steps=adjusted_ddim_steps,
         )
-        
+
         result = processed_chunk  # Keep the result as is, no channel selection
-        
+
         # Normalize the processed chunk's amplitude relative to input chunk
         result = normalize_chunk_amplitude(result, chunk)
-        
+
         # For the last chunk, ensure the output length matches the input length
         if is_last_chunk and target_length is not None:
             # Calculate the scale factor between input and output
             scale_factor = len(result) / len(chunk)
             target_output_length = int(target_length * scale_factor)
-            
+
             # Find the actual end of audio content
             audio_end = detect_audio_end(result, sr)
-            
+
             # Use the minimum of detected end and target length
             end_point = min(audio_end, target_output_length)
             result = result[:end_point]
-            
-            print(f"Adjusted last chunk length from {len(result)} to {end_point} samples")
-        
+
+            logger.info(
+                f"Adjusted last chunk length from {len(result)} to {end_point} samples"
+            )
+
         return result
-    
+
     finally:
         # Clean up: remove the temporary file
         try:
             if os.path.exists(temp_path):
                 os.remove(temp_path)
         except Exception as e:
-            print(f"Warning: Could not remove temporary file {temp_path}: {e}")
+            logger.info(f"Warning: Could not remove temporary file {temp_path}: {e}")
+
 
 def process_audio_channel(audiosr, audio_channel, sr, guidance_scale, ddim_steps):
     """Process a single audio channel"""
@@ -119,91 +142,108 @@ def process_audio_channel(audiosr, audio_channel, sr, guidance_scale, ddim_steps
     chunk_size = int(chunk_duration * sr)
     overlap_duration = 0.5  # 500ms overlap
     overlap_size = int(overlap_duration * sr)
-    
+
     # Process audio in chunks
     processed_chunks = []
-    
+
     # Calculate number of chunks
     total_samples = len(audio_channel)
     num_chunks = int(np.ceil(total_samples / (chunk_size - overlap_size)))
-    
-    print(f"Total chunks to process: {num_chunks}")
-    
+
+    logger.info(f"Total chunks to process: {num_chunks}")
+
     for i in range(num_chunks):
         # Calculate chunk boundaries
         start = i * (chunk_size - overlap_size)
         end = min(start + chunk_size, total_samples)
-        
-        print(f"\nProcessing chunk {i+1}/{num_chunks} with Audio Super Resolution")
-        print(f"Chunk time range: {start/sr:.2f}s to {end/sr:.2f}s of total {total_samples/sr:.2f}s")
-        print(f"Chunk size: {(end-start)/sr:.2f} seconds")
-        
+
+        logger.info(
+            f"\nProcessing chunk {i + 1}/{num_chunks} with Audio Super Resolution"
+        )
+        logger.info(
+            f"Chunk time range: {start / sr:.2f}s to {end / sr:.2f}s of total {total_samples / sr:.2f}s"
+        )
+        logger.info(f"Chunk size: {(end - start) / sr:.2f} seconds")
+
         # Extract chunk
         chunk = audio_channel[start:end]
-        
+
         # Check if this is the last chunk
-        is_last_chunk = (i == num_chunks - 1)
-        
+        is_last_chunk = i == num_chunks - 1
+
         # Process chunk
         # For last chunk, pass the actual remaining length
         if is_last_chunk:
             remaining_samples = total_samples - start
-            processed_chunk = process_chunk(audiosr, chunk, sr, guidance_scale, ddim_steps, 
-                                         is_last_chunk=True, target_length=remaining_samples)
+            processed_chunk = process_chunk(
+                audiosr,
+                chunk,
+                sr,
+                guidance_scale,
+                ddim_steps,
+                is_last_chunk=True,
+                target_length=remaining_samples,
+            )
         else:
-            processed_chunk = process_chunk(audiosr, chunk, sr, guidance_scale, ddim_steps, 
-                                         is_last_chunk=False)
-        
+            processed_chunk = process_chunk(
+                audiosr, chunk, sr, guidance_scale, ddim_steps, is_last_chunk=False
+            )
+
         # Ensure processed chunk is 2D by removing any singleton dimensions
         processed_chunk = np.squeeze(processed_chunk)
         if len(processed_chunk.shape) == 1:
             processed_chunk = processed_chunk.reshape(1, -1)
-        
+
         # Apply crossfade for overlapping regions (except for first chunk)
         if i > 0:
-            print(f"Applying crossfade with previous chunk (overlap: {overlap_duration}s)")
-            
+            logger.info(
+                f"Applying crossfade with previous chunk (overlap: {overlap_duration}s)"
+            )
+
             # Calculate the actual overlap size based on the processed chunk size
             scale_factor = processed_chunk.shape[1] / len(chunk)
             actual_overlap_size = int(overlap_size * scale_factor)
-            
+
             # Create fade curves with the correct size
             fade_in = np.linspace(0, 1, actual_overlap_size)
             fade_out = np.linspace(1, 0, actual_overlap_size)
-            
+
             # Reshape fade curves to match the processed chunk dimensions
             fade_in = fade_in.reshape(1, -1)
             fade_out = fade_out.reshape(1, -1)
-            
+
             # Get the overlapping regions
             current_overlap = processed_chunk[:, :actual_overlap_size]
             previous_overlap = processed_chunks[-1][:, -actual_overlap_size:]
-            
+
             # Calculate average RMS of the overlapping regions
             current_rms = np.sqrt(np.mean(np.square(current_overlap)))
             previous_rms = np.sqrt(np.mean(np.square(previous_overlap)))
-            
+
             # Adjust fade curves based on RMS ratio to maintain energy consistency
             if current_rms > 0 and previous_rms > 0:
                 rms_ratio = np.sqrt(previous_rms / current_rms)
                 fade_in = fade_in * rms_ratio
-            
+
             # Apply crossfade
             processed_chunk[:, :actual_overlap_size] *= fade_in
             processed_chunks[-1][:, -actual_overlap_size:] *= fade_out
-            
+
             # Add overlapping regions
-            processed_chunks[-1][:, -actual_overlap_size:] += processed_chunk[:, :actual_overlap_size]
+            processed_chunks[-1][:, -actual_overlap_size:] += processed_chunk[
+                :, :actual_overlap_size
+            ]
             processed_chunk = processed_chunk[:, actual_overlap_size:]
-        
+
         processed_chunks.append(processed_chunk)
-        print(f"Chunk {i+1} processed successfully")
-        print(f"Processed chunk shape: {processed_chunk.shape}")
-    
+        logger.info(f"Chunk {i + 1} processed successfully")
+        logger.info(f"Processed chunk shape: {processed_chunk.shape}")
+
     # Concatenate all processed chunks along the time axis (axis=1)
-    print("\nConcatenating processed chunks...")
+    logger.info("\nConcatenating processed chunks...")
     return np.concatenate(processed_chunks, axis=1)
 
+
 def normalize_audio(audio):
     """Normalize audio to be within [-1, 1] range"""
     max_val = np.max(np.abs(audio))
@@ -211,6 +251,7 @@ def normalize_audio(audio):
         return audio / max_val
     return audio
 
+
 def convert_audio_for_gradio(audio):
     """Convert audio to the format expected by Gradio"""
     # Ensure audio is in float32 format
@@ -222,45 +263,53 @@ def convert_audio_for_gradio(audio):
         audio = audio.T
     return audio
 
+
 def inference(audio_file, model_name, guidance_scale, ddim_steps):
     # Initialize the model
     audiosr = build_model(model_name=model_name)
-    
+
     # Load the audio file with original number of channels
     audio, sr = librosa.load(audio_file, sr=48000, mono=False)
-    
+
     # Convert to stereo if mono
     if len(audio.shape) == 1:
         audio = np.stack([audio, audio])
-    
-    print(f"\nProcessing audio file of length: {audio.shape[1]/sr:.2f} seconds")
-    print(f"Number of channels: {audio.shape[0]}")
-    
+
+    logger.info(f"\nProcessing audio file of length: {audio.shape[1] / sr:.2f} seconds")
+    logger.info(f"Number of channels: {audio.shape[0]}")
+
     # Process each channel separately
     processed_channels = []
     for channel_idx in range(audio.shape[0]):
-        print(f"\nProcessing channel {channel_idx + 1}")
+        logger.info(f"\nProcessing channel {channel_idx + 1}")
         channel_audio = audio[channel_idx]
-        processed_channel = process_audio_channel(audiosr, channel_audio, sr, guidance_scale, ddim_steps)
+        processed_channel = process_audio_channel(
+            audiosr, channel_audio, sr, guidance_scale, ddim_steps
+        )
         # Ensure the channel is 1D
         processed_channel = processed_channel.squeeze()
         processed_channels.append(processed_channel)
-    
+
     # Stack channels for stereo output (shape will be [2, samples])
     if len(processed_channels[0].shape) > 1:
         # If channels are 2D, take the first row
-        processed_channels = [channel[0] if len(channel.shape) > 1 else channel for channel in processed_channels]
-    
+        processed_channels = [
+            channel[0] if len(channel.shape) > 1 else channel
+            for channel in processed_channels
+        ]
+
     final_audio = np.stack(processed_channels)
-    
+
     # Convert audio to the format expected by Gradio
     final_audio = convert_audio_for_gradio(final_audio)
-    
-    print(f"Final audio shape: {final_audio.shape}")
-    print(f"Final audio length: {final_audio.shape[0]/sr:.2f} seconds")
-    print(f"Audio value range: [{final_audio.min():.3f}, {final_audio.max():.3f}]")
-    print(f"Audio dtype: {final_audio.dtype}")
-    
+
+    logger.info(f"Final audio shape: {final_audio.shape}")
+    logger.info(f"Final audio length: {final_audio.shape[0] / sr:.2f} seconds")
+    logger.info(
+        f"Audio value range: [{final_audio.min():.3f}, {final_audio.max():.3f}]"
+    )
+    logger.info(f"Audio dtype: {final_audio.dtype}")
+
     # Clean up temporary directory
     temp_dir = os.path.join(os.getcwd(), "temp_audio")
     try:
@@ -268,25 +317,26 @@ def inference(audio_file, model_name, guidance_scale, ddim_steps):
             os.remove(os.path.join(temp_dir, file))
         os.rmdir(temp_dir)
     except Exception as e:
-        print(f"Warning: Could not clean up temporary directory: {e}")
-    
+        logger.info(f"Warning: Could not clean up temporary directory: {e}")
+
     return (48000, final_audio)
 
+
 iface = gr.Interface(
-    fn=inference, 
+    fn=inference,
     inputs=[
         gr.Audio(type="filepath", label="Input Audio"),
         gr.Dropdown(["basic", "speech"], value="basic", label="Model"),
-        gr.Slider(1, 10, value=2.6, step=0.1, label="Guidance Scale"),  
-        gr.Slider(1, 100, value=100, step=1, label="DDIM Steps")
+        gr.Slider(1, 10, value=2.6, step=0.1, label="Guidance Scale"),
+        gr.Slider(1, 100, value=100, step=1, label="DDIM Steps"),
     ],
     outputs=gr.Audio(type="numpy", label="Output Audio"),
     title="AudioSR",
-    description="Audio Super Resolution with AudioSR"
+    description="Audio Super Resolution with AudioSR",
 )
 
 # Create temp directory on startup
 temp_dir = os.path.join(os.getcwd(), "temp_audio")
 os.makedirs(temp_dir, exist_ok=True)
 
-iface.launch()
\ No newline at end of file
+iface.launch()
diff --git a/audiosr/clap/open_clip/timm_model.py b/audiosr/clap/open_clip/timm_model.py
index 9467d91..1f66bc7 100755
--- a/audiosr/clap/open_clip/timm_model.py
+++ b/audiosr/clap/open_clip/timm_model.py
@@ -1,18 +1,18 @@
-""" timm model adapter
+"""timm model adapter
 
 Wraps timm (https://github.com/rwightman/pytorch-image-models) models for use as a vision tower in CLIP model.
 """
+
 from collections import OrderedDict
 
 import torch.nn as nn
 
 try:
     import timm
-    from timm.models.layers import Mlp, to_2tuple
-    from timm.models.layers.attention_pool2d import (
+    from timm.layers import (
         AttentionPool2d as AbsAttentionPool2d,
     )
-    from timm.models.layers.attention_pool2d import RotAttentionPool2d
+    from timm.layers import Mlp, RotAttentionPool2d, to_2tuple
 except ImportError:
     timm = None
 
diff --git a/audiosr/latent_diffusion/models/ddim.py b/audiosr/latent_diffusion/models/ddim.py
index 18a8b85..61c9f21 100755
--- a/audiosr/latent_diffusion/models/ddim.py
+++ b/audiosr/latent_diffusion/models/ddim.py
@@ -18,12 +18,27 @@ torch.backends.cudnn.allow_tf32 = True
 
 
 class DDIMSampler(object):
-    def __init__(self, model, schedule="linear", device=torch.device("cuda"), **kwargs):
+    def __init__(
+        self,
+        model,
+        schedule="linear",
+        device=torch.device("cuda"),
+        deepcache_interval=2,
+        deepcache_layer_idx=None,
+        cfg_skip_threshold=0.0,
+        freeu_args=None,
+        **kwargs,
+    ):
         super().__init__()
         self.model = model
         self.ddpm_num_timesteps = model.num_timesteps
         self.schedule = schedule
         self.device = device
+        self.deepcache_interval = deepcache_interval
+        # Sensible default for deepcache_layer_idx if none provided (around the middle)
+        self.deepcache_layer_idx = deepcache_layer_idx or 4
+        self.cfg_skip_threshold = cfg_skip_threshold
+        self.freeu_args = freeu_args
 
     def register_buffer(self, name, attr):
         if type(attr) == torch.Tensor:
@@ -253,11 +268,20 @@ class DDIMSampler(object):
                 assert len(ucg_schedule) == len(time_range)
                 unconditional_guidance_scale = ucg_schedule[i]
 
+            # DeepCache logic: Determine if this step should skip or cache
+            deepcache_skip = False
+            if self.deepcache_interval > 1:
+                if i % self.deepcache_interval == 0:
+                    deepcache_skip = False  # Full step (caching)
+                else:
+                    deepcache_skip = True  # Skip step (recycling)
+
             outs = self.p_sample_ddim(
                 img,
                 cond,
                 ts,
                 index=index,
+                total_steps=total_steps,
                 use_original_steps=ddim_use_original_steps,
                 quantize_denoised=quantize_denoised,
                 temperature=temperature,
@@ -267,6 +291,7 @@ class DDIMSampler(object):
                 unconditional_guidance_scale=unconditional_guidance_scale,
                 unconditional_conditioning=unconditional_conditioning,
                 dynamic_threshold=dynamic_threshold,
+                deepcache_skip=deepcache_skip,
             )
             img, pred_x0 = outs
             if callback:
@@ -287,6 +312,7 @@ class DDIMSampler(object):
         c,
         t,
         index,
+        total_steps=None,
         repeat_noise=False,
         use_original_steps=False,
         quantize_denoised=False,
@@ -297,27 +323,57 @@ class DDIMSampler(object):
         unconditional_guidance_scale=1.0,
         unconditional_conditioning=None,
         dynamic_threshold=None,
+        deepcache_skip=False,
     ):
         b, *_, device = *x.shape, x.device
 
         if unconditional_conditioning is None or unconditional_guidance_scale == 1.0:
-            model_output = self.model.apply_model(x, t, c)
-        else:
-            x_in = x
-            t_in = t
-
-            assert isinstance(c, dict)
-            assert isinstance(unconditional_conditioning, dict)
-
-            model_t = self.model.apply_model(x_in, t_in, c)
-
-            model_uncond = self.model.apply_model(
-                x_in, t_in, unconditional_conditioning
-            )
-
-            model_output = model_uncond + unconditional_guidance_scale * (
-                model_t - model_uncond
+            model_output = self.model.apply_model(
+                x,
+                t,
+                c,
+                deepcache_skip=deepcache_skip,
+                deepcache_layer_idx=self.deepcache_layer_idx,
+                freeu_args=self.freeu_args,
             )
+        else:
+            # Dynamic CFG Skipping: Skip unconditional pass in late denoising stages
+            # index goes from total_steps-1 to 0.
+            # Denoising is finished at index=0.
+            skip_cfg = False
+            if total_steps is not None and self.cfg_skip_threshold > 0:
+                if index < total_steps * (1.0 - self.cfg_skip_threshold):
+                    skip_cfg = True
+
+            if skip_cfg:
+                model_output = self.model.apply_model(
+                    x,
+                    t,
+                    c,
+                    deepcache_skip=deepcache_skip,
+                    deepcache_layer_idx=self.deepcache_layer_idx,
+                    freeu_args=self.freeu_args,
+                )
+            else:
+                model_t = self.model.apply_model(
+                    x,
+                    t,
+                    c,
+                    deepcache_skip=deepcache_skip,
+                    deepcache_layer_idx=self.deepcache_layer_idx,
+                    freeu_args=self.freeu_args,
+                )
+                model_uncond = self.model.apply_model(
+                    x,
+                    t,
+                    unconditional_conditioning,
+                    deepcache_skip=deepcache_skip,
+                    deepcache_layer_idx=self.deepcache_layer_idx,
+                    freeu_args=self.freeu_args,
+                )
+                model_output = model_uncond + unconditional_guidance_scale * (
+                    model_t - model_uncond
+                )
 
         if self.model.parameterization == "v":
             e_t = self.model.predict_eps_from_z_and_v(x, t, model_output)
@@ -403,7 +459,7 @@ class DDIMSampler(object):
 
         x_next = x0
         intermediates = []
-        inter_steps: list[Unknown] = []
+        inter_steps: list[int] = []
         for i in tqdm(range(num_steps), desc="Encoding Image"):
             t = torch.full(
                 (x0.shape[0],), i, device=self.model.device, dtype=torch.long
diff --git a/audiosr/latent_diffusion/models/ddpm.py b/audiosr/latent_diffusion/models/ddpm.py
index 1846d55..359b778 100755
--- a/audiosr/latent_diffusion/models/ddpm.py
+++ b/audiosr/latent_diffusion/models/ddpm.py
@@ -2,6 +2,7 @@ import os
 from contextlib import contextmanager
 from functools import partial
 from multiprocessing.sharedctypes import Value
+from typing import Any, Dict, List, Optional, Tuple, Union
 
 import librosa
 import numpy as np
@@ -78,7 +79,7 @@ class DDPM(nn.Module):
         learn_logvar=False,
         logvar_init=0.0,
         evaluator=None,
-        device=None,
+        device: Optional[Union[str, torch.device]] = None,
     ):
         super().__init__()
         assert parameterization in [
@@ -213,9 +214,9 @@ class DDPM(nn.Module):
         self.num_timesteps = int(timesteps)
         self.linear_start = linear_start
         self.linear_end = linear_end
-        assert (
-            alphas_cumprod.shape[0] == self.num_timesteps
-        ), "alphas have to be defined for each timestep"
+        assert alphas_cumprod.shape[0] == self.num_timesteps, (
+            "alphas have to be defined for each timestep"
+        )
 
         to_torch = partial(torch.tensor, dtype=torch.float32)
 
@@ -233,10 +234,12 @@ class DDPM(nn.Module):
             "log_one_minus_alphas_cumprod", to_torch(np.log(1.0 - alphas_cumprod))
         )
         self.register_buffer(
-            "sqrt_recip_alphas_cumprod", to_torch(np.sqrt(1.0 / (alphas_cumprod + epsilon)))
+            "sqrt_recip_alphas_cumprod",
+            to_torch(np.sqrt(1.0 / (alphas_cumprod + epsilon))),
         )
         self.register_buffer(
-            "sqrt_recipm1_alphas_cumprod", to_torch(np.sqrt(1.0 / (alphas_cumprod + epsilon) - 1))
+            "sqrt_recipm1_alphas_cumprod",
+            to_torch(np.sqrt(1.0 / (alphas_cumprod + epsilon) - 1)),
         )
 
         # calculations for posterior q(x_{t-1} | x_t, x_0)
@@ -916,16 +919,17 @@ class LatentDiffusion(DDPM):
         return decoding
 
     def mel_spectrogram_to_waveform(
-        self, mel, savepath=".", bs=None, name="outwav", save=True
+        self, mel, savepath=".", bs=None, name="outwav", save=True, return_numpy=True
     ):
         # Mel: [bs, 1, t-steps, fbins]
         if len(mel.size()) == 4:
             mel = mel.squeeze(1)
         mel = mel.permute(0, 2, 1)
         waveform = self.first_stage_model.vocoder(mel)
-        waveform = waveform.cpu().detach().numpy()
-        if save:
-            self.save_waveform(waveform, savepath, name)
+        if return_numpy:
+            waveform = waveform.cpu().detach().numpy()
+            if save:
+                self.save_waveform(waveform, savepath, name)
         return waveform
 
     def encode_first_stage(self, x):
@@ -1022,10 +1026,12 @@ class LatentDiffusion(DDPM):
             new_cond_dict[key] = cond_dict[key]
         return new_cond_dict
 
-    def apply_model(self, x_noisy, t, cond, return_ids=False):
+    def apply_model(self, x_noisy, t, cond, return_ids=False, **kwargs):
         cond = self.reorder_cond_dict(cond)
 
-        x_recon = self.model(x_noisy, t, cond_dict=cond)
+        # Pass specific optimizations (DeepCache, FreeU etc.) down to DiffusionWrapper
+        # and then to the UNet.
+        x_recon = self.model(x_noisy, t, cond_dict=cond, **kwargs)
 
         if isinstance(x_recon, tuple) and not return_ids:
             return x_recon[0]
@@ -1425,7 +1431,7 @@ class LatentDiffusion(DDPM):
 
         intermediate = None
         if ddim and not use_plms:
-            ddim_sampler = DDIMSampler(self, device=self.device)
+            ddim_sampler = DDIMSampler(self, device=self.device, **kwargs)
             samples, intermediates = ddim_sampler.sample(
                 ddim_steps,
                 batch_size,
@@ -1493,15 +1499,15 @@ class LatentDiffusion(DDPM):
                 self.first_stage_key,
                 unconditional_prob_cfg=0.0,  # Do not output unconditional information in the c
             )
-            self.latent_t_size = z.size(-2)
+            # Unconditional guidance handling
+            # ...
 
             c = self.filter_useful_cond_dict(c)
 
             # Generate multiple samples
             batch_size = z.shape[0] * n_gen
 
-            # Generate multiple samples at a time and filter out the best
-            # The condition to the diffusion wrapper can have many format
+            # Expanding conditions
             for cond_key in c.keys():
                 if isinstance(c[cond_key], list):
                     for i in range(len(c[cond_key])):
@@ -1537,74 +1543,117 @@ class LatentDiffusion(DDPM):
             mel = self.mel_replace_ops(mel, super().get_input(batch, "lowpass_mel"))
 
             waveform = self.mel_spectrogram_to_waveform(
-                mel, savepath="", bs=None, save=False
+                mel, savepath="", bs=None, save=False, return_numpy=False
             )
 
             waveform_lowpass = super().get_input(batch, "waveform_lowpass")
             waveform = self.postprocessing(waveform, waveform_lowpass)
 
-            max_amp = np.max(np.abs(waveform), axis=-1)
-            waveform = 0.5 * waveform / max_amp[..., None]
-            mean_amp = np.mean(waveform, axis=-1)[..., None]
-            waveform = waveform - mean_amp
+            max_amp = torch.max(torch.abs(waveform), dim=-1, keepdim=True).values
+            waveform = 0.5 * waveform / (max_amp + 1e-8)
+            mean_amp = torch.mean(waveform, dim=-1, keepdim=True)
+            waveform = (waveform - mean_amp).cpu().detach().numpy()
 
             return waveform
 
     def _locate_cutoff_freq(self, stft, percentile=0.985):
-        def _find_cutoff(x, percentile=0.95):
-            percentile = x[-1] * percentile
-            for i in range(1, x.shape[0]):
-                if x[-i] < percentile:
-                    return x.shape[0] - i
-            return 0
-
+        # stft: (B, T, F) or (B, 1, T, F)
+        if stft.ndim == 4:
+            stft = stft.squeeze(1)
         magnitude = torch.abs(stft)
-        energy = torch.cumsum(torch.sum(magnitude, dim=0), dim=0)
-        return _find_cutoff(energy, percentile)
+        energy = torch.cumsum(torch.sum(magnitude, dim=1), dim=1)  # (B, F)
+        threshold = energy[:, -1:] * percentile
+        # Find first index where energy >= threshold
+        # Find first index where energy >= threshold
+        cutoff_bins = (energy >= threshold).long().argmax(dim=1)
+        return cutoff_bins
 
     def mel_replace_ops(self, samples, input):
-        for i in range(samples.size(0)):
-            cutoff_melbin = self._locate_cutoff_freq(torch.exp(input[i]))
-
-            # ratio = samples[i][...,:cutoff_melbin]/input[i][...,:cutoff_melbin]
-            # print(torch.mean(ratio), torch.max(ratio), torch.min(ratio))
-
-            samples[i][..., :cutoff_melbin] = input[i][..., :cutoff_melbin]
+        # samples: (B, 1, T, F), input: (B, 1, T, F)
+        cutoff_bins = self._locate_cutoff_freq(torch.exp(input))
+        B, C, T, F = samples.shape
+        mask = torch.arange(F, device=samples.device).view(
+            1, 1, 1, F
+        ) < cutoff_bins.view(B, 1, 1, 1)
+        samples = torch.where(mask, input, samples)
         return samples
 
     def postprocessing(self, out_batch, x_batch):  # x is target
-        # Replace the low resolution part with the ground truth
-        for i in range(out_batch.shape[0]):
-            out = out_batch[i, 0]
-            x = x_batch[i, 0].cpu().numpy()
-            cutoffratio = self._get_cutoff_index_np(x)
-
-            length = out.shape[0]
-            stft_gt = librosa.stft(x)
-
-            stft_out = librosa.stft(out)
-            energy_ratio = np.mean(
-                np.sum(np.abs(stft_gt[cutoffratio]))
-                / np.sum(np.abs(stft_out[cutoffratio, ...]))
-            )
-            energy_ratio = min(max(energy_ratio, 0.8), 1.2)
-            stft_out[:cutoffratio, ...] = stft_gt[:cutoffratio, ...] / energy_ratio
-
-            out_renewed = librosa.istft(stft_out, length=length)
-            out_batch[i] = out_renewed
-        return out_batch
-
-    def _find_cutoff_np(self, x, threshold=0.95):
-        threshold = x[-1] * threshold
-        for i in range(1, x.shape[0]):
-            if x[-i] < threshold:
-                return x.shape[0] - i
-        return 0
+        # out_batch: (B, 1, T), x_batch: (B, 1, T)
+        # Ensure input is tensor
+        if isinstance(out_batch, np.ndarray):
+            out_batch = torch.from_numpy(out_batch).to(self.device).float()
+        if isinstance(x_batch, np.ndarray):
+            x_batch = torch.from_numpy(x_batch).to(self.device).float()
+
+        B, C, T = out_batch.shape
+        out = out_batch.squeeze(1)
+        x = x_batch.squeeze(1).to(out.device)
+
+        # STFT params matching librosa defaults
+        n_fft = 2048
+        hop_length = 512
+        window = torch.hann_window(n_fft, device=out.device)
+
+        cutoff_indices, stft_gt = self._get_cutoff_index_torch(x)
+
+        stft_out = torch.stft(
+            out,
+            n_fft=n_fft,
+            hop_length=hop_length,
+            window=window,
+            return_complex=True,
+            center=True,
+        )
 
-    def _get_cutoff_index_np(self, x):
-        stft_x = np.abs(librosa.stft(x))
-        energy = np.cumsum(np.sum(stft_x, axis=-1))
-        return self._find_cutoff_np(energy, 0.985)
+        # energy_ratio: (B,)
+        m_gt = torch.abs(stft_gt)
+        m_out = torch.abs(stft_out)
+        energy_gt = m_gt[torch.arange(B), cutoff_indices, :].sum(dim=-1)
+        energy_out = m_out[torch.arange(B), cutoff_indices, :].sum(dim=-1)
+
+        energy_ratio = energy_gt / (energy_out + 1e-8)
+        energy_ratio = energy_ratio.clamp(0.8, 1.2).view(B, 1, 1)
+
+        # Replacement mask: (B, F, 1)
+        F = stft_out.shape[1]
+        freq_indices = torch.arange(F, device=out.device).view(1, F, 1)
+        mask = freq_indices < cutoff_indices.view(B, 1, 1)
+
+        stft_out = torch.where(mask, stft_gt / energy_ratio, stft_out)
+
+        out_renewed = torch.istft(
+            stft_out,
+            n_fft=n_fft,
+            hop_length=hop_length,
+            window=window,
+            length=T,
+            center=True,
+        )
+        return out_renewed.unsqueeze(1)
+
+    def _get_cutoff_index_torch(self, x):
+        # x: (B, T) or (B, 1, T)
+        if x.ndim == 3:
+            x = x.squeeze(1)
+
+        n_fft = 2048
+        hop_length = 512
+        window = torch.hann_window(n_fft, device=x.device)
+        stft_x = torch.stft(
+            x,
+            n_fft=n_fft,
+            hop_length=hop_length,
+            window=window,
+            return_complex=True,
+            center=True,
+        )
+        magnitude = torch.abs(stft_x)
+        # energy: (B, F)
+        energy = torch.cumsum(torch.sum(magnitude, dim=-1), dim=-1)
+        threshold = energy[:, -1:] * 0.985
+        cutoff_indices = (energy >= threshold).argmax(dim=-1)
+        return cutoff_indices, stft_x
 
 
 class DiffusionWrapper(nn.Module):
@@ -1631,7 +1680,7 @@ class DiffusionWrapper(nn.Module):
 
         self.being_verbosed_once = False
 
-    def forward(self, x, t, cond_dict: dict = {}):
+    def forward(self, x, t, cond_dict: dict = {}, **kwargs):
         x = x.contiguous()
         t = t.contiguous()
 
@@ -1682,7 +1731,12 @@ class DiffusionWrapper(nn.Module):
                 raise NotImplementedError()
 
         out = self.diffusion_model(
-            xc, t, context_list=context_list, y=y, context_attn_mask_list=attn_mask_list
+            xc,
+            t,
+            context_list=context_list,
+            y=y,
+            context_attn_mask_list=attn_mask_list,
+            **kwargs,
         )
         return out
 
diff --git a/audiosr/latent_diffusion/modules/audiomae/AudioMAE.py b/audiosr/latent_diffusion/modules/audiomae/AudioMAE.py
index 1557b5a..e603150 100755
--- a/audiosr/latent_diffusion/modules/audiomae/AudioMAE.py
+++ b/audiosr/latent_diffusion/modules/audiomae/AudioMAE.py
@@ -4,7 +4,7 @@ Reference Repo: https://github.com/facebookresearch/AudioMAE
 
 import torch
 import torch.nn as nn
-from timm.models.layers import to_2tuple
+from timm.layers import to_2tuple
 
 import audiosr.latent_diffusion.modules.audiomae.models_mae as models_mae
 import audiosr.latent_diffusion.modules.audiomae.models_vit as models_vit
diff --git a/audiosr/latent_diffusion/modules/audiomae/util/patch_embed.py b/audiosr/latent_diffusion/modules/audiomae/util/patch_embed.py
index ac1e4d4..a7eae2c 100755
--- a/audiosr/latent_diffusion/modules/audiomae/util/patch_embed.py
+++ b/audiosr/latent_diffusion/modules/audiomae/util/patch_embed.py
@@ -1,6 +1,6 @@
 import torch
 import torch.nn as nn
-from timm.models.layers import to_2tuple
+from timm.layers import to_2tuple
 
 
 class PatchEmbed_org(nn.Module):
diff --git a/audiosr/latent_diffusion/modules/diffusionmodules/openaimodel.py b/audiosr/latent_diffusion/modules/diffusionmodules/openaimodel.py
index 1255cdf..16583ca 100755
--- a/audiosr/latent_diffusion/modules/diffusionmodules/openaimodel.py
+++ b/audiosr/latent_diffusion/modules/diffusionmodules/openaimodel.py
@@ -320,9 +320,9 @@ class AttentionBlock(nn.Module):
         if num_head_channels == -1:
             self.num_heads = num_heads
         else:
-            assert (
-                channels % num_head_channels == 0
-            ), f"q,k,v channels {channels} is not divisible by num_head_channels {num_head_channels}"
+            assert channels % num_head_channels == 0, (
+                f"q,k,v channels {channels} is not divisible by num_head_channels {num_head_channels}"
+            )
             self.num_heads = channels // num_head_channels
         self.use_checkpoint = use_checkpoint
         self.norm = normalization(channels)
@@ -507,14 +507,14 @@ class UNetModel(nn.Module):
             num_heads_upsample = num_heads
 
         if num_heads == -1:
-            assert (
-                num_head_channels != -1
-            ), "Either num_heads or num_head_channels has to be set"
+            assert num_head_channels != -1, (
+                "Either num_heads or num_head_channels has to be set"
+            )
 
         if num_head_channels == -1:
-            assert (
-                num_heads != -1
-            ), "Either num_heads or num_head_channels has to be set"
+            assert num_heads != -1, (
+                "Either num_heads or num_head_channels has to be set"
+            )
 
         self.image_size = image_size
         self.in_channels = in_channels
@@ -557,9 +557,9 @@ class UNetModel(nn.Module):
             )
 
         if context_dim is not None and not use_spatial_transformer:
-            assert (
-                use_spatial_transformer
-            ), "Fool!! You forgot to use the spatial transformer for your cross-attention conditioning..."
+            assert use_spatial_transformer, (
+                "Fool!! You forgot to use the spatial transformer for your cross-attention conditioning..."
+            )
 
         if context_dim is not None and not isinstance(context_dim, list):
             context_dim = [context_dim]
@@ -834,6 +834,39 @@ class UNetModel(nn.Module):
         self.middle_block.apply(convert_module_to_f32)
         self.output_blocks.apply(convert_module_to_f32)
 
+    def _freeu_fft_filter(self, x, scale, threshold=1.0):
+        # x: [B, C, T, F] or [B, C, L]
+        if x.ndim == 4:
+            B, C, T, F = x.shape
+            x_fft = th.fft.fftn(x, dim=(-2, -1))
+            x_fft = th.fft.fftshift(x_fft, dim=(-2, -1))
+
+            mask = th.ones((T, F), device=x.device)
+            crow, ccol = T // 2, F // 2
+            mask[
+                crow - int(crow * threshold) : crow + int(crow * threshold),
+                ccol - int(ccol * threshold) : ccol + int(ccol * threshold),
+            ] = scale
+            x_fft = x_fft * mask[None, None, :, :]
+
+            x_fft = th.fft.ifftshift(x_fft, dim=(-2, -1))
+            x_filtered = th.fft.ifftn(x_fft, dim=(-2, -1)).real
+        else:
+            # 1D case (common in some latent audio models)
+            B, C, L = x.shape
+            x_fft = th.fft.fft(x, dim=-1)
+            x_fft = th.fft.fftshift(x_fft, dim=-1)
+
+            mask = th.ones(L, device=x.device)
+            mid = L // 2
+            mask[mid - int(mid * threshold) : mid + int(mid * threshold)] = scale
+            x_fft = x_fft * mask[None, None, :]
+
+            x_fft = th.fft.ifftshift(x_fft, dim=-1)
+            x_filtered = th.fft.ifft(x_fft, dim=-1).real
+
+        return x_filtered
+
     def forward(
         self,
         x,
@@ -841,6 +874,9 @@ class UNetModel(nn.Module):
         y=None,
         context_list=None,
         context_attn_mask_list=None,
+        freeu_args=None,
+        deepcache_skip=False,
+        deepcache_layer_idx=None,  # Index in input_blocks where to split
         **kwargs,
     ):
         """
@@ -849,6 +885,8 @@ class UNetModel(nn.Module):
         :param timesteps: a 1-D batch of timesteps.
         :param context: conditioning plugged in via crossattn
         :param y: an [N] Tensor of labels, if class-conditional. an [N, extra_film_condition_dim] Tensor if film-embed conditional
+        :param deepcache_skip: whether to skip the interior of the UNet using cached features.
+        :param deepcache_layer_idx: at which input_block index to split for caching.
         :return: an [N x C x ...] Tensor of outputs.
         """
         if not self.shape_reported:
@@ -857,7 +895,9 @@ class UNetModel(nn.Module):
 
         assert (y is not None) == (
             self.num_classes is not None or self.extra_film_condition_dim is not None
-        ), "must specify y if and only if the model is class-conditional or film embedding conditional"
+        ), (
+            "must specify y if and only if the model is class-conditional or film embedding conditional"
+        )
         hs = []
         t_emb = timestep_embedding(timesteps, self.model_channels, repeat_only=False)
         emb = self.time_embed(t_emb)
@@ -870,12 +910,73 @@ class UNetModel(nn.Module):
             emb = th.cat([emb, self.film_emb(y)], dim=-1)
 
         h = x.type(self.dtype)
-        for module in self.input_blocks:
-            h = module(h, emb, context_list, context_attn_mask_list)
-            hs.append(h)
-        h = self.middle_block(h, emb, context_list, context_attn_mask_list)
-        for module in self.output_blocks:
+
+        if deepcache_skip and getattr(self, "_deepcache_cache", None) is not None:
+            # Fast pass: only process shallow input blocks
+            for i in range(deepcache_layer_idx):
+                h = self.input_blocks[i](h, emb, context_list, context_attn_mask_list)
+                hs.append(h)
+
+            # Load cached deep features
+            h = self._deepcache_cache["h"]
+            # Important: hs_part should contain hs[deepcache_layer_idx:]
+            hs.extend(self._deepcache_cache["hs_part"])
+
+            # Start from the corresponding shallow output block
+            start_output_idx = len(self.output_blocks) - deepcache_layer_idx
+        else:
+            # Full pass
+            for i, module in enumerate(self.input_blocks):
+                h = module(h, emb, context_list, context_attn_mask_list)
+                hs.append(h)
+
+            h = self.middle_block(h, emb, context_list, context_attn_mask_list)
+
+            if deepcache_layer_idx is not None:
+                # Cache the state after processing the "deep" part of the UNet
+                mid_output_idx = len(self.output_blocks) - deepcache_layer_idx
+                for i in range(mid_output_idx):
+                    concate_tensor = hs.pop()
+                    h = th.cat([h, concate_tensor], dim=1)
+                    h = self.output_blocks[i](
+                        h, emb, context_list, context_attn_mask_list
+                    )
+
+                # Store cache: intermediate latent h and the shallow skip connections
+                self._deepcache_cache = {
+                    "h": h.detach(),
+                    "hs_part": [t.detach() for t in hs[deepcache_layer_idx:]],
+                }
+                start_output_idx = mid_output_idx
+            else:
+                start_output_idx = 0
+
+        # FreeU parameters extraction
+        if freeu_args is not None:
+            b1, b2, s1, s2 = (
+                freeu_args.get("b1", 1.0),
+                freeu_args.get("b2", 1.0),
+                freeu_args.get("s1", 1.0),
+                freeu_args.get("s2", 1.0),
+            )
+        else:
+            b1, b2, s1, s2 = 1.0, 1.0, 1.0, 1.0
+
+        for i in range(start_output_idx, len(self.output_blocks)):
+            module = self.output_blocks[i]
             concate_tensor = hs.pop()
+
+            # FreeU logic: Scale backbone and skip features for early upsampling stages
+            if freeu_args is not None:
+                # Level 3 (lowest res) and Level 2 in 4-level UNet
+                # Assuming standard AudioSR structure with 3 blocks per level
+                if i < (self.num_res_blocks + 1):  # Level 3 (8x down)
+                    h[:, : h.shape[1] // 2] = h[:, : h.shape[1] // 2] * b1
+                    concate_tensor = self._freeu_fft_filter(concate_tensor, s1)
+                elif i < 2 * (self.num_res_blocks + 1):  # Level 2 (4x down)
+                    h[:, : h.shape[1] // 2] = h[:, : h.shape[1] // 2] * b2
+                    concate_tensor = self._freeu_fft_filter(concate_tensor, s2)
+
             h = th.cat([h, concate_tensor], dim=1)
             h = module(h, emb, context_list, context_attn_mask_list)
         h = h.type(x.dtype)
diff --git a/audiosr/pipeline.py b/audiosr/pipeline.py
index c870dab..23e224e 100755
--- a/audiosr/pipeline.py
+++ b/audiosr/pipeline.py
@@ -1,6 +1,7 @@
 import gc
 import os
 import re
+from typing import Any, Dict, List, Optional, Tuple, Union
 
 import numpy as np
 import soundfile as sf
@@ -8,6 +9,8 @@ import torch
 import torch.nn.functional as F
 import torchaudio
 import yaml
+from loguru import logger
+from pyinstrument import Profiler
 
 import audiosr.latent_diffusion.modules.phoneme_encoder.text as text
 from audiosr.latent_diffusion.models.ddpm import LatentDiffusion
@@ -23,7 +26,7 @@ from audiosr.utils import (
 )
 
 
-def seed_everything(seed):
+def seed_everything(seed: int) -> None:
     import random
 
     import numpy as np
@@ -38,15 +41,17 @@ def seed_everything(seed):
     torch.backends.cudnn.benchmark = True
 
 
-def text2phoneme(data):
+def text2phoneme(data: str) -> str:
     return text._clean_text(re.sub(r"<.*?>", "", data), ["english_cleaners2"])
 
 
-def text_to_filename(text):
+def text_to_filename(text: str) -> str:
     return text.replace(" ", "_").replace("'", "_").replace('"', "_")
 
 
-def extract_kaldi_fbank_feature(waveform, sampling_rate, log_mel_spec):
+def extract_kaldi_fbank_feature(
+    waveform: torch.Tensor, sampling_rate: int, log_mel_spec: torch.Tensor
+) -> Dict[str, torch.Tensor]:
     norm_mean = -4.2677393
     norm_std = 4.5689974
 
@@ -85,7 +90,11 @@ def extract_kaldi_fbank_feature(waveform, sampling_rate, log_mel_spec):
     return {"ta_kaldi_fbank": fbank}  # [1024, 128]
 
 
-def make_batch_for_super_resolution(input_file, waveform=None, fbank=None):
+def make_batch_for_super_resolution(
+    input_file: str,
+    waveform: Optional[torch.Tensor] = None,
+    fbank: Optional[torch.Tensor] = None,
+) -> Tuple[Dict[str, Any], float]:
     if waveform is None:
         # Original logic if no waveform is provided
         log_mel_spec, stft, waveform, duration, target_frame = read_audio_file(
@@ -143,8 +152,12 @@ def round_up_duration(duration):
 
 
 def build_model(
-    ckpt_path=None, config=None, device=None, model_name="basic", use_safetensors=False
-):
+    ckpt_path: Optional[str] = None,
+    config: Optional[Union[str, Dict[str, Any]]] = None,
+    device: Optional[Union[str, torch.device]] = None,
+    model_name: str = "basic",
+    use_safetensors: bool = False,
+) -> LatentDiffusion:
     if device is None or device == "auto":
         if torch.cuda.is_available():
             device = torch.device("cuda:0")
@@ -152,9 +165,11 @@ def build_model(
             device = torch.device("mps")
         else:
             device = torch.device("cpu")
+    elif isinstance(device, str):
+        device = torch.device(device)
 
-    print("Loading AudioSR: %s" % model_name)
-    print("Loading model on %s" % device)
+    logger.info(f"Loading AudioSR: {model_name}")
+    logger.info(f"Loading model on {device}")
 
     # If user provided a specific path, use it. Otherwise, download/find based on name.
     if ckpt_path is None:
@@ -172,56 +187,87 @@ def build_model(
 
     # No normalization here
     latent_diffusion = LatentDiffusion(**config["model"]["params"])
+    latent_diffusion.eval()
+    latent_diffusion = latent_diffusion.to(device)
 
     # Load weights
-    print(f"Loading weights from {ckpt_path}")
+    logger.info(f"Loading weights from {ckpt_path}")
     if ckpt_path.endswith(".safetensors"):
         from safetensors.torch import load_file
 
         state_dict = load_file(ckpt_path)
         latent_diffusion.load_state_dict(state_dict, strict=False)
     else:
-        checkpoint = torch.load(ckpt_path, map_location="cpu")
+        checkpoint = torch.load(
+            ckpt_path, map_location="cpu", mmap=True, weights_only=True
+        )
         latent_diffusion.load_state_dict(checkpoint["state_dict"], strict=False)
 
-    latent_diffusion.eval()
-    latent_diffusion = latent_diffusion.to(device)
+    # Optimization: Use torch.compile for the UNet model to benefit from kernel fusion
+    # and other compiler-level optimizations. Only on PyTorch 2.0+ and CUDA/MPS.
+    torch_version = torch.__version__.split(".")
+    has_compile = int(torch_version[0]) >= 2
+    if has_compile and device.type in ["cuda", "mps"]:
+        logger.info("Compiling UNet with torch.compile...")
+        try:
+            latent_diffusion.model.diffusion_model = torch.compile(
+                latent_diffusion.model.diffusion_model
+            )
+        except Exception as e:
+            logger.warning(f"torch.compile failed, falling back to eager mode: {e}")
 
     return latent_diffusion
 
 
 def super_resolution(
-    latent_diffusion,
-    input_file,
-    seed=42,
-    ddim_steps=200,
-    guidance_scale=3.5,
-    latent_t_per_second=12.8,
-    config=None,
-):
+    latent_diffusion: LatentDiffusion,
+    input_file: str,
+    seed: int = 42,
+    ddim_steps: int = 50,
+    guidance_scale: float = 3.5,
+    latent_t_per_second: float = 12.8,
+    config: Optional[Any] = None,
+    deepcache_interval: int = 2,
+    cfg_skip_threshold: float = 0.0,
+    freeu_args: Optional[Dict[str, float]] = None,
+) -> np.ndarray:
     seed_everything(int(seed))
     waveform = None
 
     batch, duration = make_batch_for_super_resolution(input_file, waveform=waveform)
+    logger.info(f"Generating batch for duration: {duration:.2f}s")
+
+    # Use inference_mode for better performance than no_grad
+    # Use autocast for automatic mixed precision speedups
+    if isinstance(latent_diffusion.device, torch.device):
+        device_type = latent_diffusion.device.type
+    else:
+        device_type = str(latent_diffusion.device)
 
-    with torch.no_grad():
+    with torch.inference_mode(), torch.autocast(device_type):
         waveform = latent_diffusion.generate_batch(
             batch,
             unconditional_guidance_scale=guidance_scale,
             ddim_steps=ddim_steps,
             duration=duration,
+            deepcache_interval=deepcache_interval,
+            cfg_skip_threshold=cfg_skip_threshold,
+            freeu_args=freeu_args,
         )
 
     return waveform
 
 
 def super_resolution_from_waveform(
-    latent_diffusion,
-    waveform_np,
-    seed=42,
-    ddim_steps=200,
-    guidance_scale=3.5,
-):
+    latent_diffusion: LatentDiffusion,
+    waveform_np: np.ndarray,
+    seed: int = 42,
+    ddim_steps: int = 50,
+    guidance_scale: float = 3.5,
+    deepcache_interval: int = 2,
+    cfg_skip_threshold: float = 0.0,
+    freeu_args: Optional[Dict[str, float]] = None,
+) -> np.ndarray:
     """
     Process a single waveform directly without saving to file.
 
@@ -231,6 +277,9 @@ def super_resolution_from_waveform(
         seed: Random seed
         ddim_steps: Number of DDIM steps
         guidance_scale: Guidance scale for generation
+        deepcache_interval: DeepCache skipping interval (default 1 = disabled)
+        cfg_skip_threshold: CFG skipping threshold (0.0 to 1.0)
+        freeu_args: FreeU parameters (dict)
 
     Returns:
         Processed waveform as numpy array
@@ -274,16 +323,20 @@ def super_resolution_from_waveform(
 
     # Add batch dimension to all tensors
     for k in batch.keys():
-        if isinstance(batch[k], torch.Tensor):
-            batch[k] = torch.FloatTensor(batch[k]).unsqueeze(0)
+        if torch.is_tensor(batch[k]):
+            batch[k] = batch[k].unsqueeze(0)
 
     # Process
-    with torch.no_grad():
+    device_type = latent_diffusion.device.type
+    with torch.inference_mode(), torch.autocast(device_type):
         result = latent_diffusion.generate_batch(
             batch,
             unconditional_guidance_scale=guidance_scale,
             ddim_steps=ddim_steps,
             duration=pad_duration,
+            deepcache_interval=deepcache_interval,
+            cfg_skip_threshold=cfg_skip_threshold,
+            freeu_args=freeu_args,
         )
 
     # Convert to numpy and trim to original length
@@ -337,12 +390,13 @@ def _prepare_single_waveform_batch(waveform_np, target_duration, sampling_rate=4
 
 
 def super_resolution_batch(
-    latent_diffusion,
-    waveforms_list,
-    seed=42,
-    ddim_steps=200,
-    guidance_scale=3.5,
-):
+    latent_diffusion: LatentDiffusion,
+    waveforms_list: List[np.ndarray],
+    seed: int = 42,
+    ddim_steps: int = 50,
+    guidance_scale: float = 3.5,
+    **kwargs: Any,
+) -> List[np.ndarray]:
     """
     Process multiple waveform chunks in a SINGLE forward pass (true GPU parallelism).
 
@@ -369,7 +423,7 @@ def super_resolution_batch(
     else:
         target_duration = max_duration
 
-    print(
+    logger.info(
         f"[Batch] Processing {len(waveforms_list)} chunks, target_duration={target_duration:.2f}s"
     )
 
@@ -391,17 +445,23 @@ def super_resolution_batch(
             # After stack, shape becomes [batch, time, ...] or [batch, samples]
             tensors = [b[key] for b in batch_list]
             combined_batch[key] = torch.stack(tensors, dim=0)
-            print(f"  {key}: {combined_batch[key].shape}")
+            logger.debug(f"  {key}: {combined_batch[key].shape}")
         else:
             combined_batch[key] = batch_list[0][key]
 
     # Process all chunks at once
-    with torch.no_grad():
+    if isinstance(latent_diffusion.device, torch.device):
+        device_type = latent_diffusion.device.type
+    else:
+        device_type = str(latent_diffusion.device)
+
+    with torch.inference_mode(), torch.autocast(device_type):
         results = latent_diffusion.generate_batch(
             combined_batch,
             unconditional_guidance_scale=guidance_scale,
             ddim_steps=ddim_steps,
             duration=target_duration,
+            **kwargs,
         )
 
     # Convert results to numpy
@@ -421,14 +481,15 @@ def super_resolution_batch(
 
 
 def super_resolution_long_audio(
-    latent_diffusion,
-    input_file,
-    seed=42,
-    ddim_steps=200,
-    guidance_scale=3.5,
-    chunk_duration_s=15,
-    overlap_duration_s=2,
-):
+    latent_diffusion: LatentDiffusion,
+    input_file: str,
+    seed: int = 42,
+    ddim_steps: int = 50,
+    guidance_scale: float = 3.5,
+    chunk_duration_s: float = 15.0,
+    overlap_duration_s: float = 2.0,
+    **kwargs: Any,
+) -> torch.Tensor:
     """
     Processes a long audio file by chunking it, running super-resolution on each chunk,
     and reconstructing the full audio with cross-fading in overlap regions.
@@ -492,7 +553,7 @@ def super_resolution_long_audio(
             padding_needed = chunk_samples - current_chunk_len
             chunk_waveform = F.pad(chunk_waveform, (0, padding_needed))
 
-        print(
+        logger.info(
             f"Processing chunk from {start_sample / sr:.2f}s to {end_sample / sr:.2f}s"
         )
 
@@ -502,13 +563,19 @@ def super_resolution_long_audio(
             None, waveform=chunk_waveform.squeeze(0).numpy()
         )
 
-        with torch.no_grad():
+        if isinstance(latent_diffusion.device, torch.device):
+            device_type = latent_diffusion.device.type
+        else:
+            device_type = str(latent_diffusion.device)
+
+        with torch.inference_mode(), torch.autocast(device_type):
             # Run inference on the single chunk
             processed_chunk = latent_diffusion.generate_batch(
                 batch,
                 unconditional_guidance_scale=guidance_scale,
                 ddim_steps=ddim_steps,
                 duration=duration,
+                **kwargs,
             )  # This should return a tensor
 
         # Ensure the processed chunk is a tensor
diff --git a/audiosr/utils.py b/audiosr/utils.py
index 2615923..cd8f9df 100755
--- a/audiosr/utils.py
+++ b/audiosr/utils.py
@@ -76,10 +76,18 @@ def lowpass_filtering_prepare_inference(dl_output):
         _locate_cutoff_freq(dl_output["stft"], percentile=0.985) / 1024
     ) * 24000
 
-    # If the audio is almost empty. Give up processing
+    # If the audio is almost empty or cutoff is near Nyquist. Give up processing
     if cutoff_freq < 1000:
         cutoff_freq = 24000
 
+    # If calculate cutoff is essentially Nyquist, skip filtering to avoid numerical issues
+    # and unnecessary processing.
+    if cutoff_freq >= (sampling_rate / 2) - 1000:
+        return {"waveform_lowpass": torch.FloatTensor(waveform.numpy()).unsqueeze(0)}
+
+    # Ensure cutoff is strictly less than Nyquist (sr/2) for stability
+    cutoff_freq = min(cutoff_freq, (sampling_rate / 2) - 100.0)
+
     order = 8
     ftype = np.random.choice(["butter", "cheby1", "ellip", "bessel"])
     filtered_audio = lowpass(
@@ -95,7 +103,7 @@ def lowpass_filtering_prepare_inference(dl_output):
     if waveform.size(-1) <= filtered_audio.size(-1):
         filtered_audio = filtered_audio[..., : waveform.size(-1)]
     else:
-        filtered_audio = torch.functional.pad(
+        filtered_audio = torch.nn.functional.pad(
             filtered_audio, (0, waveform.size(-1) - filtered_audio.size(-1))
         )
 
@@ -126,8 +134,12 @@ def mel_spectrogram_train(y):
         )
         hann_window[str(y.device)] = torch.hann_window(win_length).to(y.device)
 
+    # Ensure y is (Batch, 1, Time) for padding
+    if y.ndim == 2:
+        y = y.unsqueeze(1)
+
     y = torch.nn.functional.pad(
-        y.unsqueeze(1),
+        y,
         (int((filter_length - hop_length) / 2), int((filter_length - hop_length) / 2)),
         mode="reflect",
     )
diff --git a/inference.py b/inference.py
index 0f40b09..e726b5f 100644
--- a/inference.py
+++ b/inference.py
@@ -21,18 +21,26 @@ warnings.filterwarnings("ignore")
 os.environ["TOKENIZERS_PARALLELISM"] = "true"
 torch.set_float32_matmul_precision("high")
 
-def match_array_shapes(array_1:np.ndarray, array_2:np.ndarray):
+
+def match_array_shapes(array_1: np.ndarray, array_2: np.ndarray):
     if (len(array_1.shape) == 1) & (len(array_2.shape) == 1):
         if array_1.shape[0] > array_2.shape[0]:
-            array_1 = array_1[:array_2.shape[0]]
+            array_1 = array_1[: array_2.shape[0]]
         elif array_1.shape[0] < array_2.shape[0]:
-            array_1 = np.pad(array_1, ((array_2.shape[0] - array_1.shape[0], 0)), 'constant', constant_values=0)
+            array_1 = np.pad(
+                array_1,
+                ((array_2.shape[0] - array_1.shape[0], 0)),
+                "constant",
+                constant_values=0,
+            )
     else:
         if array_1.shape[1] > array_2.shape[1]:
-            array_1 = array_1[:,:array_2.shape[1]]
+            array_1 = array_1[:, : array_2.shape[1]]
         elif array_1.shape[1] < array_2.shape[1]:
             padding = array_2.shape[1] - array_1.shape[1]
-            array_1 = np.pad(array_1, ((0,0), (0,padding)), 'constant', constant_values=0)
+            array_1 = np.pad(
+                array_1, ((0, 0), (0, padding)), "constant", constant_values=0
+            )
     return array_1
 
 
@@ -40,11 +48,12 @@ def lr_filter(audio, cutoff, filter_type, order=12, sr=48000):
     audio = audio.T
     nyquist = 0.5 * sr
     normal_cutoff = cutoff / nyquist
-    b, a = signal.butter(order//2, normal_cutoff, btype=filter_type, analog=False)
+    b, a = signal.butter(order // 2, normal_cutoff, btype=filter_type, analog=False)
     sos = signal.tf2sos(b, a)
     filtered_audio = signal.sosfiltfilt(sos, audio)
     return filtered_audio.T
 
+
 class Predictor(BasePredictor):
     def setup(self, model_name="basic", device="auto"):
         self.model_name = model_name
@@ -56,13 +65,21 @@ class Predictor(BasePredictor):
         # exit()
         print("Model loaded!")
 
-    def process_audio(self, input_file, chunk_size=5.12, overlap=0.1, seed=None, guidance_scale=3.5, ddim_steps=50):
+    def process_audio(
+        self,
+        input_file,
+        chunk_size=5.12,
+        overlap=0.1,
+        seed=None,
+        guidance_scale=3.5,
+        ddim_steps=50,
+    ):
         audio, sr = librosa.load(input_file, sr=input_cutoff * 2, mono=False)
         audio = audio.T
         sr = input_cutoff * 2
         print(f"audio.shape = {audio.shape}")
         print(f"input cutoff = {input_cutoff}")
-        
+
         is_stereo = len(audio.shape) == 2
         audio_channels = [audio] if not is_stereo else [audio[:, 0], audio[:, 1]]
         print("audio is stereo" if is_stereo else "Audio is mono")
@@ -73,7 +90,7 @@ class Predictor(BasePredictor):
         output_overlap_samples = int(overlap * output_chunk_samples)
         enable_overlap = overlap > 0
         print(f"enable_overlap = {enable_overlap}")
-        
+
         def process_chunks(audio):
             chunks = []
             original_lengths = []
@@ -83,49 +100,63 @@ class Predictor(BasePredictor):
                 chunk = audio[start:end]
                 if len(chunk) < chunk_samples:
                     original_lengths.append(len(chunk))
-                    chunk = np.concatenate([chunk, np.zeros(chunk_samples - len(chunk))])
+                    chunk = np.concatenate(
+                        [chunk, np.zeros(chunk_samples - len(chunk))]
+                    )
                 else:
                     original_lengths.append(chunk_samples)
                 chunks.append(chunk)
-                start += chunk_samples - overlap_samples if enable_overlap else chunk_samples
+                start += (
+                    chunk_samples - overlap_samples if enable_overlap else chunk_samples
+                )
             return chunks, original_lengths
 
         # Process both channels (mono or stereo)
         chunks_per_channel = [process_chunks(channel) for channel in audio_channels]
         sample_rate_ratio = self.sr / sr
-        total_length = len(chunks_per_channel[0][0]) * output_chunk_samples - (len(chunks_per_channel[0][0]) - 1) * (output_overlap_samples if enable_overlap else 0)
+        total_length = len(chunks_per_channel[0][0]) * output_chunk_samples - (
+            len(chunks_per_channel[0][0]) - 1
+        ) * (output_overlap_samples if enable_overlap else 0)
         reconstructed_channels = [np.zeros((1, total_length)) for _ in audio_channels]
 
         meter_before = pyln.Meter(sr)
         meter_after = pyln.Meter(self.sr)
-        
+
         # Process chunks for each channel
         for ch_idx, (chunks, original_lengths) in enumerate(chunks_per_channel):
             for i, chunk in enumerate(chunks):
                 loudness_before = meter_before.integrated_loudness(chunk)
-                print(f"Processing chunk {i+1} of {len(chunks)} for {'Left/Mono' if ch_idx == 0 else 'Right'} channel")
-                with tempfile.NamedTemporaryFile(suffix=".wav", delete=True) as temp_wav:
+                print(
+                    f"Processing chunk {i + 1} of {len(chunks)} for {'Left/Mono' if ch_idx == 0 else 'Right'} channel"
+                )
+                with tempfile.NamedTemporaryFile(
+                    suffix=".wav", delete=True
+                ) as temp_wav:
                     sf.write(temp_wav.name, chunk, sr)
-                
+
                     out_chunk = super_resolution(
                         self.audiosr,
                         temp_wav.name,
                         seed=seed,
                         guidance_scale=guidance_scale,
                         ddim_steps=ddim_steps,
-                        latent_t_per_second=12.8
+                        latent_t_per_second=12.8,
                     )
 
                     out_chunk = out_chunk[0]
                     num_samples_to_keep = int(original_lengths[i] * sample_rate_ratio)
                     out_chunk = out_chunk[:, :num_samples_to_keep].squeeze()
                     loudness_after = meter_after.integrated_loudness(out_chunk)
-                    out_chunk = pyln.normalize.loudness(out_chunk, loudness_after, loudness_before)
+                    out_chunk = pyln.normalize.loudness(
+                        out_chunk, loudness_after, loudness_before
+                    )
 
                     if enable_overlap:
-                        actual_overlap_samples = min(output_overlap_samples, num_samples_to_keep)
-                        fade_out = np.linspace(1., 0., actual_overlap_samples)
-                        fade_in = np.linspace(0., 1., actual_overlap_samples)
+                        actual_overlap_samples = min(
+                            output_overlap_samples, num_samples_to_keep
+                        )
+                        fade_out = np.linspace(1.0, 0.0, actual_overlap_samples)
+                        fade_in = np.linspace(0.0, 1.0, actual_overlap_samples)
 
                         if i == 0:
                             out_chunk[-actual_overlap_samples:] *= fade_out
@@ -135,34 +166,50 @@ class Predictor(BasePredictor):
                         else:
                             out_chunk[:actual_overlap_samples] *= fade_in
 
-                    start = i * (output_chunk_samples - output_overlap_samples if enable_overlap else output_chunk_samples)
+                    start = i * (
+                        output_chunk_samples - output_overlap_samples
+                        if enable_overlap
+                        else output_chunk_samples
+                    )
                     end = start + out_chunk.shape[0]
                     reconstructed_channels[ch_idx][0, start:end] += out_chunk.flatten()
 
-        reconstructed_audio = np.stack(reconstructed_channels, axis=-1) if is_stereo else reconstructed_channels[0]
+        reconstructed_audio = (
+            np.stack(reconstructed_channels, axis=-1)
+            if is_stereo
+            else reconstructed_channels[0]
+        )
 
         if multiband_ensemble:
             low, _ = librosa.load(input_file, sr=48000, mono=False)
             output = match_array_shapes(reconstructed_audio[0].T, low)
-            low = lr_filter(low.T, crossover_freq, 'lowpass', order=10)
-            high = lr_filter(output.T, crossover_freq, 'highpass', order=10)
-            high = lr_filter(high, 23000, 'lowpass', order=2)
+            low = lr_filter(low.T, crossover_freq, "lowpass", order=10)
+            high = lr_filter(output.T, crossover_freq, "highpass", order=10)
+            high = lr_filter(high, 23000, "lowpass", order=2)
             output = low + high
         else:
             output = reconstructed_audio[0]
         # print(output, type(output))
         return output
 
-
-    def predict(self,
+    def predict(
+        self,
         input_file: Path = Input(description="Audio to upsample"),
-        ddim_steps: int = Input(description="Number of inference steps", default=50, ge=10, le=500),
-        guidance_scale: float = Input(description="Scale for classifier free guidance", default=3.5, ge=1.0, le=20.0),
+        ddim_steps: int = Input(
+            description="Number of inference steps", default=50, ge=10, le=500
+        ),
+        guidance_scale: float = Input(
+            description="Scale for classifier free guidance",
+            default=3.5,
+            ge=1.0,
+            le=20.0,
+        ),
         overlap: float = Input(description="overlap size", default=0.04),
         chunk_size: float = Input(description="chunksize", default=10.24),
-        seed: int = Input(description="Random seed. Leave blank to randomize the seed", default=None)
+        seed: int = Input(
+            description="Random seed. Leave blank to randomize the seed", default=None
+        ),
     ) -> Path:
-
         if seed == 0:
             seed = random.randint(0, 2**32 - 1)
         print(f"Setting seed to: {seed}")
@@ -179,11 +226,16 @@ class Predictor(BasePredictor):
             overlap=overlap,
             seed=seed,
             guidance_scale=guidance_scale,
-            ddim_steps=ddim_steps
+            ddim_steps=ddim_steps,
         )
-        
+
         filename = os.path.splitext(os.path.basename(input_file))[0]
-        sf.write(f"{output_folder}/SR_{filename}.wav", data=waveform, samplerate=48000,  subtype="PCM_16")
+        sf.write(
+            f"{output_folder}/SR_{filename}.wav",
+            data=waveform,
+            samplerate=48000,
+            subtype="PCM_16",
+        )
         print(f"file created: {output_folder}/SR_{filename}.wav")
         del self.audiosr, waveform
         gc.collect()
@@ -191,17 +243,48 @@ class Predictor(BasePredictor):
 
 
 if __name__ == "__main__":
-
-    parser = argparse.ArgumentParser(description="Find volume difference of two audio files.")
+    parser = argparse.ArgumentParser(
+        description="Find volume difference of two audio files."
+    )
     parser.add_argument("--input", help="Path to input audio file")
     parser.add_argument("--output", help="Output folder")
-    parser.add_argument("--ddim_steps", help="Number of ddim steps", type=int, required=False, default=50)
-    parser.add_argument("--chunk_size", help="chunk size", type=float, required=False, default=10.24)
-    parser.add_argument("--guidance_scale", help="Guidance scale value",  type=float, required=False, default=3.5)
-    parser.add_argument("--seed", help="Seed value, 0 = random seed", type=int, required=False, default=0)
-    parser.add_argument("--overlap", help="overlap value", type=float, required=False, default=0.04)
-    parser.add_argument("--multiband_ensemble", type=bool, help="Use multiband ensemble with input")
-    parser.add_argument("--input_cutoff", help="Define the crossover of audio input in the multiband ensemble", type=int, required=False, default=12000)
+    parser.add_argument(
+        "--ddim_steps",
+        help="Number of ddim steps",
+        type=int,
+        required=False,
+        default=50,
+    )
+    parser.add_argument(
+        "--chunk_size", help="chunk size", type=float, required=False, default=10.24
+    )
+    parser.add_argument(
+        "--guidance_scale",
+        help="Guidance scale value",
+        type=float,
+        required=False,
+        default=3.5,
+    )
+    parser.add_argument(
+        "--seed",
+        help="Seed value, 0 = random seed",
+        type=int,
+        required=False,
+        default=0,
+    )
+    parser.add_argument(
+        "--overlap", help="overlap value", type=float, required=False, default=0.04
+    )
+    parser.add_argument(
+        "--multiband_ensemble", type=bool, help="Use multiband ensemble with input"
+    )
+    parser.add_argument(
+        "--input_cutoff",
+        help="Define the crossover of audio input in the multiband ensemble",
+        type=int,
+        required=False,
+        default=12000,
+    )
 
     args = parser.parse_args()
 
@@ -218,9 +301,8 @@ if __name__ == "__main__":
     crossover_freq = input_cutoff - 1000
 
     p = Predictor()
-    
-    p.setup(device='auto')
 
+    p.setup(device="auto")
 
     out = p.predict(
         input_file_path,
@@ -228,7 +310,7 @@ if __name__ == "__main__":
         guidance_scale=guidance_scale,
         seed=seed,
         chunk_size=chunk_size,
-        overlap=overlap
+        overlap=overlap,
     )
 
     del p
diff --git a/pyproject.toml b/pyproject.toml
index 84399e0..e2e9a8c 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -32,6 +32,10 @@ dependencies = [
     "timm",
     "matplotlib",
     "diffusers",
+    "pyinstrument",
+    "loguru>=0.7.3",
+    "pyloudnorm>=0.2.0",
+    "torchcodec>=0.10.0",
 ]
 
 
diff --git a/scripts/optimize_model.py b/scripts/optimize_model.py
new file mode 100644
index 0000000..798b429
--- /dev/null
+++ b/scripts/optimize_model.py
@@ -0,0 +1,71 @@
+#!/usr/bin/env python3
+import argparse
+import os
+import torch
+from safetensors.torch import save_file
+from huggingface_hub import HfApi, create_repo
+
+
+def optimize_and_push(input_path, repo_id, push_to_hub=True):
+    print(f"Loading checkpoint from {input_path}...")
+    # Load with mmap to save RAM if possible, though we need to load into memory to convert types
+    checkpoint = torch.load(input_path, map_location="cpu")
+
+    if "state_dict" in checkpoint:
+        state_dict = checkpoint["state_dict"]
+    else:
+        state_dict = checkpoint
+
+    print(f"Optimizing weights to FP16...")
+    new_state_dict = {}
+    for key, tensor in state_dict.items():
+        if isinstance(tensor, torch.Tensor) and tensor.is_floating_point():
+            new_state_dict[key] = tensor.half()
+        else:
+            new_state_dict[key] = tensor
+
+    # Free original memory
+    del checkpoint
+    del state_dict
+
+    output_path = "audiosr.safetensors"
+    print(f"Saving to {output_path}...")
+    save_file(new_state_dict, output_path)
+
+    file_size_gb = os.path.getsize(output_path) / (1024**3)
+    print(f"Optimized model size: {file_size_gb:.2f} GB")
+
+    if push_to_hub:
+        print(f"Pushing to Hugging Face Hub: {repo_id}...")
+        api = HfApi()
+
+        # Ensure repo exists
+        try:
+            create_repo(repo_id, repo_type="model", exist_ok=True)
+        except Exception as e:
+            print(f"Note on repo creation: {e}")
+
+        api.upload_file(
+            path_or_fileobj=output_path,
+            path_in_repo="audiosr.safetensors",
+            repo_id=repo_id,
+            repo_type="model",
+        )
+        print("Upload complete!")
+    else:
+        print("Skipping upload.")
+
+
+if __name__ == "__main__":
+    parser = argparse.ArgumentParser(
+        description="Optimize AudioSR model and push to HF"
+    )
+    parser.add_argument("input_path", type=str, help="Path to input .bin or .ckpt file")
+    parser.add_argument(
+        "--repo", type=str, default="oulianov/audio-sr", help="HF Repo ID"
+    )
+    parser.add_argument("--no-push", action="store_true", help="Don't push to hub")
+
+    args = parser.parse_args()
+
+    optimize_and_push(args.input_path, args.repo, not args.no_push)
diff --git a/uv.lock b/uv.lock
index 7859ab5..b6d3f68 100644
--- a/uv.lock
+++ b/uv.lock
@@ -72,17 +72,21 @@ dependencies = [
     { name = "gradio" },
     { name = "huggingface-hub" },
     { name = "librosa" },
+    { name = "loguru" },
     { name = "matplotlib" },
     { name = "numpy" },
     { name = "pandas" },
     { name = "phonemizer" },
     { name = "progressbar" },
+    { name = "pyinstrument" },
+    { name = "pyloudnorm" },
     { name = "pyyaml" },
     { name = "scipy" },
     { name = "soundfile" },
     { name = "timm" },
     { name = "torch" },
     { name = "torchaudio" },
+    { name = "torchcodec" },
     { name = "torchlibrosa" },
     { name = "torchvision" },
     { name = "tqdm" },
@@ -111,11 +115,14 @@ requires-dist = [
     { name = "gradio" },
     { name = "huggingface-hub" },
     { name = "librosa" },
+    { name = "loguru", specifier = ">=0.7.3" },
     { name = "matplotlib" },
     { name = "numpy" },
     { name = "pandas" },
     { name = "phonemizer" },
     { name = "progressbar" },
+    { name = "pyinstrument" },
+    { name = "pyloudnorm", specifier = ">=0.2.0" },
     { name = "pyyaml" },
     { name = "ruff", marker = "extra == 'dev'", specifier = ">=0.1.0" },
     { name = "scipy" },
@@ -123,6 +130,7 @@ requires-dist = [
     { name = "timm" },
     { name = "torch", specifier = ">=2.0.1" },
     { name = "torchaudio", specifier = ">=2.0.2" },
+    { name = "torchcodec", specifier = ">=0.10.0" },
     { name = "torchlibrosa", specifier = ">=0.0.9" },
     { name = "torchvision", specifier = ">=0.15.2" },
     { name = "tqdm" },
@@ -738,6 +746,19 @@ wheels = [
     { url = "https://files.pythonhosted.org/packages/19/0c/8f5a37a65fc9b7b17408508145edd5f86263ad69c19d3574e818f533a0eb/llvmlite-0.46.0-cp311-cp311-win_amd64.whl", hash = "sha256:e8b10bc585c58bdffec9e0c309bb7d51be1f2f15e169a4b4d42f2389e431eb93", size = 38138652, upload-time = "2025-12-08T18:14:58.171Z" },
 ]
 
+[[package]]
+name = "loguru"
+version = "0.7.3"
+source = { registry = "https://pypi.org/simple" }
+dependencies = [
+    { name = "colorama", marker = "sys_platform == 'win32'" },
+    { name = "win32-setctime", marker = "sys_platform == 'win32'" },
+]
+sdist = { url = "https://files.pythonhosted.org/packages/3a/05/a1dae3dffd1116099471c643b8924f5aa6524411dc6c63fdae648c4f1aca/loguru-0.7.3.tar.gz", hash = "sha256:19480589e77d47b8d85b2c827ad95d49bf31b0dcde16593892eb51dd18706eb6", size = 63559, upload-time = "2024-12-06T11:20:56.608Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/0c/29/0348de65b8cc732daa3e33e67806420b2ae89bdce2b04af740289c5c6c8c/loguru-0.7.3-py3-none-any.whl", hash = "sha256:31a33c10c8e1e10422bfd431aeb5d351c7cf7fa671e3c4df004162264b28220c", size = 61595, upload-time = "2024-12-06T11:20:54.538Z" },
+]
+
 [[package]]
 name = "markdown-it-py"
 version = "4.0.0"
@@ -1208,6 +1229,35 @@ wheels = [
     { url = "https://files.pythonhosted.org/packages/c7/21/705964c7812476f378728bdf590ca4b771ec72385c533964653c68e86bdc/pygments-2.19.2-py3-none-any.whl", hash = "sha256:86540386c03d588bb81d44bc3928634ff26449851e99741617ecb9037ee5ec0b", size = 1225217, upload-time = "2025-06-21T13:39:07.939Z" },
 ]
 
+[[package]]
+name = "pyinstrument"
+version = "5.1.2"
+source = { registry = "https://pypi.org/simple" }
+sdist = { url = "https://files.pythonhosted.org/packages/32/7f/d3c4ef7c43f3294bd5a475dfa6f295a9fee5243c292d5c8122044fa83bcb/pyinstrument-5.1.2.tar.gz", hash = "sha256:af149d672da9493fa37334a1cc68f7b80c3e6cb9fd99b9e426c447db5c650bf0", size = 266889, upload-time = "2026-01-04T18:38:58.464Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/79/ef/0288edd620fb0cf2074d8c8e3567007a6bac66307b839d99988563de4eb8/pyinstrument-5.1.2-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:3739a05583ea6312c385eb59fe985cd20d9048e95f9eeeb6a2f6c35202e2d36e", size = 131284, upload-time = "2026-01-04T18:37:35.01Z" },
+    { url = "https://files.pythonhosted.org/packages/0b/4e/2a90a6997d9f7a39a6998d56de72e52673ebf5a9169a1c39dbf173e95105/pyinstrument-5.1.2-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:7c9ee05dc75ac5fb18498c311e624f77f7f321f7ff325b251aa09e52e46f1d6a", size = 124468, upload-time = "2026-01-04T18:37:36.628Z" },
+    { url = "https://files.pythonhosted.org/packages/04/74/7bfd403e81f9b5ec523f60cced8f516ee52312752bb2e0fafabfd90bbd78/pyinstrument-5.1.2-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:7a49a55ca5b75218767e29cacbe515d0b66fc18cb48a937bca0f77b8dafc7202", size = 148057, upload-time = "2026-01-04T18:37:37.998Z" },
+    { url = "https://files.pythonhosted.org/packages/50/3a/7205d7c199947d18edcd013af4ddf4d3cca85c5488fbe493050035947f7c/pyinstrument-5.1.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:0c45c14974ff04b1bfdc6c2a448627c6da7409c7800d0eb7bd03fb435dcb41d7", size = 146526, upload-time = "2026-01-04T18:37:39.642Z" },
+    { url = "https://files.pythonhosted.org/packages/24/e8/f6864172e7ebe4bc5209bafbc574a619b4c511b9506b941789b11441be7c/pyinstrument-5.1.2-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:22b9c04b3982c41c04b1c5ed05d1bc3a2ba26533450084058119f6dc160e70a3", size = 147179, upload-time = "2026-01-04T18:37:41.332Z" },
+    { url = "https://files.pythonhosted.org/packages/6d/04/89ef2d1c34767bfdbcc74ab0c7e0d021d7fac5e79873239e4ca26e97d6da/pyinstrument-5.1.2-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:5c4995ee0774801790c138f0dfec17d4e7a7ef09a6d56d53cbcbf0578a711021", size = 146354, upload-time = "2026-01-04T18:37:42.808Z" },
+    { url = "https://files.pythonhosted.org/packages/2e/d4/64441547ec12391b92c739a3b0685059e7dfa088d928df8364676ef7abc7/pyinstrument-5.1.2-cp311-cp311-win32.whl", hash = "sha256:fe449e4a8ee60a2a27cf509350a584670f4c3704649601be7937598f09dbe7ca", size = 125790, upload-time = "2026-01-04T18:37:44.141Z" },
+    { url = "https://files.pythonhosted.org/packages/4d/8b/0a5f6b239294decb0ecd932711f3470bfbd42fc2e08a94cd5c1f4f6da7f1/pyinstrument-5.1.2-cp311-cp311-win_amd64.whl", hash = "sha256:3fb839429671a42bf349335af4c1ce5cf83386ac11f04df0bc40720d4cb7d77d", size = 126578, upload-time = "2026-01-04T18:37:45.423Z" },
+]
+
+[[package]]
+name = "pyloudnorm"
+version = "0.2.0"
+source = { registry = "https://pypi.org/simple" }
+dependencies = [
+    { name = "numpy" },
+    { name = "scipy" },
+]
+sdist = { url = "https://files.pythonhosted.org/packages/23/00/f915eaa75326f4209941179c2b93ac477f2040e4aeff5bb21d16eb8058f9/pyloudnorm-0.2.0.tar.gz", hash = "sha256:8bf597658ea4e1975c275adf490f6deb5369ea409f2901f939915efa4b681b16", size = 14037, upload-time = "2026-01-04T11:43:35.265Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/aa/b6/65a49a05614b2548edbba3aab118f2ebe7441dfd778accdcdce9f6567f20/pyloudnorm-0.2.0-py3-none-any.whl", hash = "sha256:9bb69afb904f59d007a7f9ba3d75d16fb8aeef35c44d6df822a9f192d69cf13f", size = 10879, upload-time = "2026-01-04T11:43:34.534Z" },
+]
+
 [[package]]
 name = "pyparsing"
 version = "3.3.2"
@@ -1709,6 +1759,16 @@ wheels = [
     { url = "https://files.pythonhosted.org/packages/69/26/cd2aec609b4f8918e4e85e5c6a3f569bc7b5f72a7ecba3f784077102749c/torchaudio-2.10.0-cp311-cp311-win_amd64.whl", hash = "sha256:4c6e9609046143b30a30183893d23ff1ce5de603dbe914b3cce5cc29f5aa5a9c", size = 474792, upload-time = "2026-01-21T16:28:45.254Z" },
 ]
 
+[[package]]
+name = "torchcodec"
+version = "0.10.0"
+source = { registry = "https://pypi.org/simple" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/1c/0d/51ab5cb4ba8eb60e3e39651a4d43be89a592cc193fe11feb6509509b0121/torchcodec-0.10.0-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:3dde1ebd9677ec1587f1e45486b3d59bd3e41a0bf4fc9b3dc6880e64c421ad56", size = 3907950, upload-time = "2026-01-22T15:41:43.819Z" },
+    { url = "https://files.pythonhosted.org/packages/b9/c8/618bde55c1908583290883537326174e633a383a8337226ce0c7c6d70090/torchcodec-0.10.0-cp311-cp311-manylinux_2_28_x86_64.whl", hash = "sha256:2e2be11c4468a58940572fcf5f8ed5e41187c1de214267f692e2fd5ac8731198", size = 2070483, upload-time = "2026-01-22T15:41:36.743Z" },
+    { url = "https://files.pythonhosted.org/packages/74/f1/75d70391de0581069864b3c204bb7e463a2b3b32e840ff9d103688a6db94/torchcodec-0.10.0-cp311-cp311-win_amd64.whl", hash = "sha256:f51d9435d3c75c0b55f5dc64a22ce9cfb58ac19013cdd8ce572a523ef75e2b58", size = 2219702, upload-time = "2026-01-22T15:41:50.933Z" },
+]
+
 [[package]]
 name = "torchlibrosa"
 version = "0.1.0"
@@ -1910,6 +1970,15 @@ wheels = [
     { url = "https://files.pythonhosted.org/packages/f2/3e/45583b67c2ff08ad5a582d316fcb2f11d6cf0a50c7707ac09d212d25bc98/wcwidth-0.5.0-py3-none-any.whl", hash = "sha256:1efe1361b83b0ff7877b81ba57c8562c99cf812158b778988ce17ec061095695", size = 93772, upload-time = "2026-01-27T01:31:43.432Z" },
 ]
 
+[[package]]
+name = "win32-setctime"
+version = "1.2.0"
+source = { registry = "https://pypi.org/simple" }
+sdist = { url = "https://files.pythonhosted.org/packages/b3/8f/705086c9d734d3b663af0e9bb3d4de6578d08f46b1b101c2442fd9aecaa2/win32_setctime-1.2.0.tar.gz", hash = "sha256:ae1fdf948f5640aae05c511ade119313fb6a30d7eabe25fef9764dca5873c4c0", size = 4867, upload-time = "2024-12-07T15:28:28.314Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/e1/07/c6fe3ad3e685340704d314d765b7912993bcb8dc198f0e7a89382d37974b/win32_setctime-1.2.0-py3-none-any.whl", hash = "sha256:95d644c4e708aba81dc3704a116d8cbc974d70b3bdb8be1d150e36be6e9d1390", size = 4083, upload-time = "2024-12-07T15:28:26.465Z" },
+]
+
 [[package]]
 name = "zipp"
 version = "3.23.0"
